{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jbRd8fChq9R-"},"source":["## This is the test script of CoE202 final project.\n","\n","There're two functions, 1) visualizing coloring example for training dataset 2) evaluation for test dataset.\n","\n","You can just execute the **\"Evaluation\"** cell if you just want to test your model.\n","\n","(of course, you should import libraries/modules and set the file path)\n"]},{"cell_type":"code","metadata":{"id":"uA5cZcbzM_g4"},"source":["import os\n","import sys\n","import time\n","import datetime\n","from PIL import Image\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import torch\n","import torch.nn as nn\n","from torchvision.utils import save_image, make_grid\n","import torchvision.transforms as transforms\n","import glob\n","\n","\n","!rm /etc/localtime\n","!ln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8xmIwMONAzY","executionInfo":{"status":"ok","timestamp":1623684292368,"user_tz":-540,"elapsed":23812,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"02bd824b-792e-4ca0-f80d-b2da9cc1a14e"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# Autoreload in case that the custom modules are changed\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MUTgzsyBNB3l"},"source":["filepath = \"/content/drive/MyDrive/\"\n","\n","sys.path.append(filepath)\n","\n","from util import *\n","from model_baseline import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXKQ2QM-Elm3"},"source":["# Class information\n","classes = ['background',\n","'aeroplane',\n","'bicycle',\n","'boat',\n","'bus',\n","'car',\n","'motorbike',\n","'person',\n","'train',\n","'boundary'] # For convenience, the classes are denoted as corresponding indexes (0~9) in this script.\n","# We ignore the boundary class (index : 9) in this project!\n","\n","image_transform = transforms.ToPILImage() # converting Torch.tensor -> PIL format\n","\n","# You can freely add or modify any transform functions,\n","# except ToTensor, ToLabel, Relabel functions (again, do not change!).\n","\n","common_transform = []\n","\n","input_transform = [transforms.ToTensor()]\n","\n","target_transform = [ToLabel(),\n","    Relabel(4, 3),\n","    Relabel(6, 4),\n","    Relabel(7, 5),\n","    Relabel(14, 6),\n","    Relabel(15, 7),\n","    Relabel(19, 8),\n","    Relabel(255, 9),\n","]\n","\n","input_transform = transforms.Compose(common_transform + input_transform)\n","target_transform = transforms.Compose(common_transform + target_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlulGqA7Pc88","executionInfo":{"status":"ok","timestamp":1623684404022,"user_tz":-540,"elapsed":12053,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"35d86192-9ae7-4432-f1f6-b5f1d5395591"},"source":["# You can modify the loss, the model and the optimizer\n","model = UNet().cuda()\n","experiment = \"exp_colab\"\n","\n","os.makedirs(f\"{filepath}/{experiment}/eval_result\", exist_ok=True)\n","\n","model.load_state_dict(torch.load(f\"{filepath}/{experiment}/best_model_state_dict.pt\")) # load your model(.pt)\n","\n","model.eval()\n","\n","print(\"Loaded.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W8PlqmAME7AU"},"source":["# Specify the path of evaluation data folder.\n","evaldata_path = \"/content/drive/MyDrive/test_data\" # path of test_dataset\n","eval_paths = glob.glob(f\"{evaldata_path}/*.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"syrc3pcfIT6A","executionInfo":{"status":"ok","timestamp":1623684524223,"user_tz":-540,"elapsed":539,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"39a57438-3d81-4588-8b20-5bb8fa72daa2"},"source":["# Make color palette for visualization. Image will be saved containing [0, 1, ... 9] labels. With this palette, we can view with color for corresponding label.\n","import matplotlib.pyplot as plt\n","\n","squares = torch.arange(10).unsqueeze(0)\n","plt.imshow(squares.numpy(), cmap='tab10')\n","a = plt.get_cmap('tab10')\n","\n","colors = torch.zeros(10, 3)\n","for i in range(1, 10):\n","    colors[i, :] = torch.tensor(list(a(i)[:3]))\n","colors = colors * 255\n","colors = colors.int().numpy().astype(\"uint8\")\n","print(colors)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[  0   0   0]\n"," [255 127  14]\n"," [ 44 160  44]\n"," [214  39  40]\n"," [148 103 189]\n"," [140  86  75]\n"," [227 119 194]\n"," [127 127 127]\n"," [188 189  34]\n"," [ 23 190 207]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAABECAYAAACCuY6+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAG70lEQVR4nO3dX4ycVR3G8e9jt7QWmlKBxNoiXZFUGxtT2SjaSEhbEoim3EAElbTGpl6IIDHxb+KFN1RjEC+IyaZoDCJiKtFqGhUtXJk0rFCDbS3UqrS1lrZA658Abn28mHfdybDrtn1n5yzzPp9ks++f0/f8crLzzPSdmXNkm4iI6H+vK11ARET0RgI/IqIhEvgREQ2RwI+IaIgEfkREQyTwIyIaolbgS3qDpEckPVP9XjhJu9OSdlU/2+r0GRER50Z1Pocv6WvA87Y3S/o8sND25yZo9w/bF9SoMyIiaqob+PuAa2wfkbQIeMz2sgnaJfAjIgqrG/gv2r6w2hbwwth+R7tRYBcwCmy2/eNJrrcJ2ASg2XOvnH3RknOurRtW6EDR/sfsnnNe6RK4/EjpClpOzX9z6RLw6aOlSwBg4Zw3li6B4zpVugQA5s8/UboEDnB56RIAGH1673Hbl0x0bmCqfyzpV8BEf1lfat+xbUmTPXtcZvuwpLcAOyQ9ZfuPnY1sDwPDAHMWXeFF6++ZqrxpNTL3w0X7H7NisHzI/fCu0dIlALDjmntLl8BLL9xdugQAPjT4qrunPbdl7q9LlwDA+6++v3QJfETfL10CAEdXr/zLZOemDHzbayc7J+mopEVtt3Sem+Qah6vfByQ9BqwEXhX4ERExfep+LHMbsL7aXg/8pLOBpIWS5lTbFwOrgD01+42IiLNUN/A3A9dKegZYW+0jaUjSlqrN24ERSb8DHqV1Dz+BHxHRY1Pe0vl/bJ8A1kxwfATYWG3/BlhRp5+IiKgv37SNiGiIBH5EREMk8CMiGiKBHxHREAn8iIiGSOBHRDREAj8ioiES+BERDdGVwJd0naR9kvZX8+J3np8j6aHq/E5JS7vRb0REnLnagS9pFnAvcD2wHLhF0vKOZh+nNXXyW4FvAF+t229ERJydbrzCfzew3/YB268APwBu6GhzA/DdansrsKaaPz8iInqkG4G/GDjYtn+oOjZhG9ujwEngos4LSdokaUTSyOl/nexCaRERMWZGvWlre9j2kO2hWfMWlC4nIqKvdCPwDwOXtu0vqY5N2EbSALAAKL8mWUREg3Qj8B8HrpA0KOk84GZaC6O0a18o5UZgh+ssphsREWet1nz40LonL+k24BfALODbtndL+gowYnsbcB9wv6T9wPO0nhQiIqKHagc+gO3twPaOY19u234JuKkbfUVExLmZUW/aRkTE9EngR0Q0RAI/IqIhEvgREQ2RwI+IaIgEfkREQyTwIyIaIoEfEdEQvVoAZYOkY5J2VT8bu9FvREScudrftG1bAOVaWlMjPy5pm+09HU0fsn1b3f4iIuLc9GoBlIiIKEx1J62UdCNwne2N1f6twHvaX81L2gDcBRwDngbutH1wgmttAjZVu8uAfbWKg4uB4zWv0S8yFuMyFuMyFuP6ZSwus33JRCe6MnnaGfgp8KDtlyV9gtZyh6s7G9keBoa71amkEdtD3brea1nGYlzGYlzGYlwTxqInC6DYPmH75Wp3C3BlF/qNiIiz0JMFUCQtattdB+ztQr8REXEWerUAyu2S1gGjtBZA2VC33zPUtdtDfSBjMS5jMS5jMa7vx6L2m7YREfHakG/aRkQ0RAI/IqIh+jbwp5ruoSkkXSrpUUl7JO2WdEfpmkqSNEvSk5J+VrqW0iRdKGmrpD9I2ivpvaVrKkXSndXj4/eSHpQ0t3RN06EvA79tuofrgeXALZKWl62qmFHgM7aXA1cBn2zwWADcQT4lNuabwM9tvw14Jw0dF0mLgduBIdvvoPXhk5vLVjU9+jLwyXQP/2P7iO0nqu2/03pQLy5bVRmSlgAfoPVdkEaTtAC4GrgPwPYrtl8sW1VRA8DrJQ0A84C/Fq5nWvRr4C8G2qduOERDQ66dpKXASmBn2UqKuQf4LPCf0oXMAIO0pjr5TnWLa4uk80sXVYLtw8DXgWeBI8BJ278sW9X06NfAjw6SLgB+BHza9qnS9fSapA8Cz9n+belaZogB4F3At2yvBP4JNPK9LkkLad0BGATeBJwv6aNlq5oe/Rr4U0730CSSZtMK+wdsP1y6nkJWAesk/ZnWLb7Vkr5XtqSiDgGHbI/9b28rrSeAJloL/Mn2Mdv/Bh4G3le4pmnRr4E/5XQPTSFJtO7T7rV9d+l6SrH9BdtLbC+l9feww3Zfvoo7E7b/BhyUtKw6tAboXMOiKZ4FrpI0r3q8rKFP38Du1WyZPTXZdA+FyyplFXAr8JSkXdWxL9reXrCmmBk+BTxQvSg6AHyscD1F2N4paSvwBK1PtT1Jn06zkKkVIiIaol9v6URERIcEfkREQyTwIyIaIoEfEdEQCfyIiIZI4EdENEQCPyKiIf4L3FwtOWxXMMcAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"L4kHIoSMqyuA"},"source":["### **Coloring example for training dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"KhOaquM4brTC","executionInfo":{"status":"error","timestamp":1623684529803,"user_tz":-540,"elapsed":422,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"b5bd6fee-34f1-45e0-c15d-ce42b16b5695"},"source":["# Because we will use except boundary\n","colors = colors[:9]\n","\n","train_dataset = CoE_Dataset(root=f\"{filepath}/train_dataset\",\n","                input_transform=input_transform,\n","                target_transform=target_transform)\n","\n","\n","print('Total number of images : {}'.format(len(train_dataset)))\n","\n","\n","# ------------\n","# Load dataset\n","# ------------\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                    batch_size=1,\n","                                    shuffle=True,\n","                                    drop_last=True\n","                                    )\n","\n","\n","fig = plt.figure(figsize=(12, 14))\n","for i in range(2):\n","    for j in range(2):\n","        (sample_batch, sample_label, _) = next(iter(train_loader))\n","        fig.add_subplot(4, 4, (i * 4 + j * 2 + 1))\n","        plt.imshow(sample_batch[0].permute(1, 2, 0))\n","        classes_name = [classes[index] for index in torch.unique(sample_label[0]) if not index == 9]\n","        plt.title('class : ' + str(classes_name))\n","        fig.add_subplot(4, 4, (i * 4 + j * 2 + 1+ 1))\n","\n","        r = Image.fromarray(sample_label[0][0].byte().cpu().numpy())\n","        r.putpalette(colors)\n","        r.convert('RGB')\n","        plt.imshow(r)\n","plt.suptitle(\"Sample images\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e69acedd4a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m train_dataset = CoE_Dataset(root=f\"{filepath}/train_dataset\",\n\u001b[1;32m      5\u001b[0m                 \u001b[0minput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 target_transform=target_transform)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, input_transform, target_transform, train)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         self.filenames = [image_basename(f)\n\u001b[0;32m---> 94\u001b[0;31m             for f in os.listdir(self.labels_root) if is_image(f)]\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive//train_dataset/Labels'"]}]},{"cell_type":"markdown","metadata":{"id":"LOlCwKVsbb9C"},"source":["### **EVALUATION**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr7-rkKcGPdS","executionInfo":{"status":"ok","timestamp":1623684598781,"user_tz":-540,"elapsed":59777,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"111ab1b6-ec22-4bc7-9ffd-356ec527939f"},"source":["# We will iterate the evaluation data and inference with loaded model.\n","# Predicted images will save in this path. \"{filepath}/{experiment}/eval_result/\"\n","\n","# Because we will use except boundary\n","colors = colors[:9]\n","\n","for eval_path in eval_paths:\n","    name = eval_path.split('/')[-1].split('.')[0]\n","    img = Image.open(eval_path)\n","    img = input_transform(img).unsqueeze(0)\n","    print(f\"name : {name}, Size : {img.size()}\")\n","\n","    img = img.cuda()\n","    pred = model(img)\n","\n","    pred = pred[0].max(0)[1].unsqueeze(0).detach()\n","\n","    # Because we will use except boundary\n","    pred[pred==9]==0\n","    \n","    r = Image.fromarray(pred[0].byte().cpu().numpy())\n","    r.putpalette(colors)\n","    r.save(f\"{filepath}/{experiment}/eval_result/{name}.png\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["name : 3231560638, Size : torch.Size([1, 3, 176, 500])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3672: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["name : 3231560370, Size : torch.Size([1, 3, 500, 398])\n","name : 3231561620, Size : torch.Size([1, 3, 333, 500])\n","name : 3231561439, Size : torch.Size([1, 3, 335, 500])\n","name : 3231562052, Size : torch.Size([1, 3, 375, 500])\n","name : 3231563022, Size : torch.Size([1, 3, 333, 500])\n","name : 3231562104, Size : torch.Size([1, 3, 375, 500])\n","name : 3231563239, Size : torch.Size([1, 3, 375, 500])\n","name : 3231563692, Size : torch.Size([1, 3, 350, 500])\n","name : 3231563736, Size : torch.Size([1, 3, 333, 500])\n","name : 3231564131, Size : torch.Size([1, 3, 375, 500])\n","name : 3231564381, Size : torch.Size([1, 3, 500, 333])\n","name : 3231565598, Size : torch.Size([1, 3, 375, 500])\n","name : 3231566317, Size : torch.Size([1, 3, 375, 500])\n","name : 3231566546, Size : torch.Size([1, 3, 375, 500])\n","name : 3231567362, Size : torch.Size([1, 3, 331, 500])\n","name : 3231567453, Size : torch.Size([1, 3, 500, 422])\n","name : 3231567612, Size : torch.Size([1, 3, 374, 500])\n","name : 3231567832, Size : torch.Size([1, 3, 335, 500])\n","name : 3231568078, Size : torch.Size([1, 3, 342, 500])\n","name : 3231568213, Size : torch.Size([1, 3, 333, 500])\n","name : 3231568385, Size : torch.Size([1, 3, 375, 500])\n","name : 3231568488, Size : torch.Size([1, 3, 375, 500])\n","name : 3231568551, Size : torch.Size([1, 3, 333, 500])\n","name : 3231569166, Size : torch.Size([1, 3, 333, 500])\n","name : 3231569509, Size : torch.Size([1, 3, 374, 500])\n","name : 3231569613, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560000, Size : torch.Size([1, 3, 500, 375])\n","name : 3232560098, Size : torch.Size([1, 3, 500, 328])\n","name : 3232560152, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560163, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560219, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560559, Size : torch.Size([1, 3, 311, 500])\n","name : 3232560569, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560591, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560646, Size : torch.Size([1, 3, 375, 500])\n","name : 3232560664, Size : torch.Size([1, 3, 336, 346])\n","name : 3232560703, Size : torch.Size([1, 3, 500, 332])\n","name : 3232560737, Size : torch.Size([1, 3, 333, 500])\n","name : 3232560824, Size : torch.Size([1, 3, 333, 500])\n","name : 3232561135, Size : torch.Size([1, 3, 374, 500])\n","name : 3232561548, Size : torch.Size([1, 3, 375, 500])\n","name : 3232561804, Size : torch.Size([1, 3, 375, 500])\n","name : 3232562035, Size : torch.Size([1, 3, 419, 343])\n","name : 3232562466, Size : torch.Size([1, 3, 375, 500])\n","name : 3232563019, Size : torch.Size([1, 3, 333, 500])\n","name : 3232563845, Size : torch.Size([1, 3, 334, 500])\n","name : 3232564103, Size : torch.Size([1, 3, 333, 500])\n","name : 3232565053, Size : torch.Size([1, 3, 482, 500])\n","name : 3232565301, Size : torch.Size([1, 3, 333, 500])\n","name : 3232567044, Size : torch.Size([1, 3, 500, 338])\n","name : 3232567212, Size : torch.Size([1, 3, 500, 375])\n","name : 3232567385, Size : torch.Size([1, 3, 500, 412])\n","name : 3232567572, Size : torch.Size([1, 3, 500, 333])\n","name : 3232567987, Size : torch.Size([1, 3, 333, 500])\n","name : 3232568292, Size : torch.Size([1, 3, 500, 333])\n","name : 3232568394, Size : torch.Size([1, 3, 500, 375])\n","name : 3232568370, Size : torch.Size([1, 3, 333, 500])\n","name : 3232568664, Size : torch.Size([1, 3, 375, 500])\n","name : 3232568716, Size : torch.Size([1, 3, 333, 500])\n","name : 3232568864, Size : torch.Size([1, 3, 500, 354])\n","name : 3232569038, Size : torch.Size([1, 3, 333, 500])\n","name : 3232569509, Size : torch.Size([1, 3, 333, 500])\n","name : 3232569913, Size : torch.Size([1, 3, 375, 500])\n","name : 3233560090, Size : torch.Size([1, 3, 327, 500])\n","name : 3233560426, Size : torch.Size([1, 3, 375, 500])\n","name : 3233560597, Size : torch.Size([1, 3, 333, 500])\n","name : 3233560761, Size : torch.Size([1, 3, 500, 375])\n","name : 3233560844, Size : torch.Size([1, 3, 375, 500])\n","name : 3233560954, Size : torch.Size([1, 3, 375, 500])\n","name : 3233560983, Size : torch.Size([1, 3, 375, 500])\n","name : 3233561045, Size : torch.Size([1, 3, 500, 375])\n","name : 3233561380, Size : torch.Size([1, 3, 332, 480])\n","name : 3233561770, Size : torch.Size([1, 3, 335, 500])\n","name : 3233562037, Size : torch.Size([1, 3, 333, 500])\n","name : 3233567125, Size : torch.Size([1, 3, 333, 500])\n","name : 3233567144, Size : torch.Size([1, 3, 375, 500])\n","name : 3233567230, Size : torch.Size([1, 3, 366, 500])\n","name : 3233567236, Size : torch.Size([1, 3, 500, 376])\n","name : 3233567247, Size : torch.Size([1, 3, 333, 500])\n","name : 3233567480, Size : torch.Size([1, 3, 344, 500])\n","name : 3233567812, Size : torch.Size([1, 3, 375, 500])\n","name : 3233567923, Size : torch.Size([1, 3, 273, 500])\n","name : 3233568123, Size : torch.Size([1, 3, 333, 500])\n","name : 3233568190, Size : torch.Size([1, 3, 377, 500])\n","name : 3233568480, Size : torch.Size([1, 3, 281, 500])\n","name : 3233569028, Size : torch.Size([1, 3, 409, 500])\n","name : 3233569029, Size : torch.Size([1, 3, 500, 500])\n","name : 3233569085, Size : torch.Size([1, 3, 500, 375])\n","name : 3233569110, Size : torch.Size([1, 3, 333, 500])\n","name : 3233569325, Size : torch.Size([1, 3, 347, 500])\n","name : 3233569333, Size : torch.Size([1, 3, 333, 500])\n","name : 3233569428, Size : torch.Size([1, 3, 375, 500])\n","name : 3233569452, Size : torch.Size([1, 3, 333, 500])\n","name : 3233569954, Size : torch.Size([1, 3, 340, 500])\n","name : 3244560021, Size : torch.Size([1, 3, 500, 500])\n","name : 3244560603, Size : torch.Size([1, 3, 375, 500])\n","name : 3244560701, Size : torch.Size([1, 3, 375, 500])\n","name : 3244561048, Size : torch.Size([1, 3, 375, 500])\n","name : 3244561145, Size : torch.Size([1, 3, 375, 500])\n","name : 3244561556, Size : torch.Size([1, 3, 375, 500])\n","name : 3244561731, Size : torch.Size([1, 3, 500, 333])\n","name : 3244561939, Size : torch.Size([1, 3, 328, 500])\n","name : 3244562284, Size : torch.Size([1, 3, 333, 500])\n","name : 3244562517, Size : torch.Size([1, 3, 375, 500])\n","name : 3244562590, Size : torch.Size([1, 3, 375, 500])\n","name : 3244562678, Size : torch.Size([1, 3, 375, 500])\n","name : 3244562845, Size : torch.Size([1, 3, 333, 500])\n","name : 3244562853, Size : torch.Size([1, 3, 375, 500])\n","name : 3244562996, Size : torch.Size([1, 3, 375, 500])\n","name : 3244567412, Size : torch.Size([1, 3, 375, 500])\n","name : 3244567855, Size : torch.Size([1, 3, 500, 375])\n","name : 3244567877, Size : torch.Size([1, 3, 375, 500])\n","name : 3244568103, Size : torch.Size([1, 3, 500, 375])\n","name : 3244568164, Size : torch.Size([1, 3, 336, 500])\n","name : 3244568203, Size : torch.Size([1, 3, 333, 500])\n","name : 3244568305, Size : torch.Size([1, 3, 500, 375])\n","name : 3244568409, Size : torch.Size([1, 3, 377, 500])\n","name : 3244568524, Size : torch.Size([1, 3, 375, 500])\n","name : 3244568741, Size : torch.Size([1, 3, 336, 448])\n","name : 3244568756, Size : torch.Size([1, 3, 360, 480])\n","name : 3244568964, Size : torch.Size([1, 3, 333, 500])\n","name : 3244569682, Size : torch.Size([1, 3, 333, 500])\n","name : 3244569727, Size : torch.Size([1, 3, 362, 500])\n","name : 3244569820, Size : torch.Size([1, 3, 331, 500])\n","name : 3244569940, Size : torch.Size([1, 3, 336, 500])\n","name : 3244569997, Size : torch.Size([1, 3, 333, 500])\n","name : 3245560028, Size : torch.Size([1, 3, 375, 500])\n","name : 3245560845, Size : torch.Size([1, 3, 375, 500])\n","name : 3245560893, Size : torch.Size([1, 3, 375, 500])\n","name : 3245567083, Size : torch.Size([1, 3, 333, 500])\n","name : 3245567346, Size : torch.Size([1, 3, 375, 500])\n","name : 3245567375, Size : torch.Size([1, 3, 500, 353])\n","name : 3245567379, Size : torch.Size([1, 3, 332, 500])\n","name : 3245567427, Size : torch.Size([1, 3, 375, 500])\n","name : 3245567448, Size : torch.Size([1, 3, 375, 500])\n","name : 3245567620, Size : torch.Size([1, 3, 500, 347])\n","name : 3245568131, Size : torch.Size([1, 3, 330, 500])\n","name : 3245568265, Size : torch.Size([1, 3, 500, 375])\n","name : 3245568409, Size : torch.Size([1, 3, 375, 500])\n","name : 3245568422, Size : torch.Size([1, 3, 375, 500])\n","name : 3245568543, Size : torch.Size([1, 3, 332, 500])\n","name : 3245568670, Size : torch.Size([1, 3, 375, 500])\n","name : 3245568778, Size : torch.Size([1, 3, 500, 375])\n","name : 3245568794, Size : torch.Size([1, 3, 375, 500])\n","name : 3245568854, Size : torch.Size([1, 3, 375, 500])\n","name : 3245569014, Size : torch.Size([1, 3, 500, 339])\n","name : 3245569198, Size : torch.Size([1, 3, 500, 375])\n","name : 3245569722, Size : torch.Size([1, 3, 337, 500])\n","name : 3245569765, Size : torch.Size([1, 3, 375, 500])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_2TB2hv7JQhK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623684609539,"user_tz":-540,"elapsed":451,"user":{"displayName":"이의인","photoUrl":"","userId":"03683293784995167550"}},"outputId":"eb765942-8ba2-4f44-fb58-ff5d0d8f17a5"},"source":["# While we can see the images with color, actual value of the image is within [0~8] value.\n","C = np.array(Image.open(f\"{filepath}/{experiment}/eval_result/{name}.png\").convert('P'))\n","np.unique(C)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4, 6, 7, 8], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":12}]}]}